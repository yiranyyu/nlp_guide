nohup: ignoring input

Unknown vocab size: 42399
Total words in training file: 1001247
Total bytes in training file: 6213810
Vocab size: 15669
Initializing unigram table
Filling unigram table
End filling unigram talbe
Begin training

Completed training. Training took 6.9073219180107115 minutes
Saving model to ./model/1m_words.txt_5_skip-gram_neg5.model
[1 epoch] of process 15, Alpha: 0.025000
[2 epoch] of process 15, Alpha: 0.004526
[3 epoch] of process 15, Alpha: 0.000003
[4 epoch] of process 15, Alpha: 0.000003
[5 epoch] of process 15, Alpha: 0.000003
[1 epoch] of process 14, Alpha: 0.025000
[2 epoch] of process 14, Alpha: 0.002029
[3 epoch] of process 14, Alpha: 0.000003
[4 epoch] of process 14, Alpha: 0.000003
[5 epoch] of process 14, Alpha: 0.000003
[1 epoch] of process 0, Alpha: 0.025000
[2 epoch] of process 0, Alpha: 0.003776
[3 epoch] of process 0, Alpha: 0.000003
[4 epoch] of process 0, Alpha: 0.000003
[5 epoch] of process 0, Alpha: 0.000003
[1 epoch] of process 2, Alpha: 0.025000
[2 epoch] of process 2, Alpha: 0.004276
[3 epoch] of process 2, Alpha: 0.000003
[4 epoch] of process 2, Alpha: 0.000003
[5 epoch] of process 2, Alpha: 0.000003
[1 epoch] of process 3, Alpha: 0.025000
[2 epoch] of process 3, Alpha: 0.002778
[3 epoch] of process 3, Alpha: 0.000003
[4 epoch] of process 3, Alpha: 0.000003
[5 epoch] of process 3, Alpha: 0.000003
[1 epoch] of process 13, Alpha: 0.025000
[2 epoch] of process 13, Alpha: 0.001529
[3 epoch] of process 13, Alpha: 0.000003
[4 epoch] of process 13, Alpha: 0.000003
[5 epoch] of process 13, Alpha: 0.000003
[1 epoch] of process 5, Alpha: 0.025000
[2 epoch] of process 5, Alpha: 0.004026
[3 epoch] of process 5, Alpha: 0.000003
[4 epoch] of process 5, Alpha: 0.000003
[5 epoch] of process 5, Alpha: 0.000003
[1 epoch] of process 12, Alpha: 0.025000
[2 epoch] of process 12, Alpha: 0.003027
[3 epoch] of process 12, Alpha: 0.000003
[4 epoch] of process 12, Alpha: 0.000003
[5 epoch] of process 12, Alpha: 0.000003
[1 epoch] of process 10, Alpha: 0.025000
[2 epoch] of process 10, Alpha: 0.002278
[3 epoch] of process 10, Alpha: 0.000003
[4 epoch] of process 10, Alpha: 0.000003
[5 epoch] of process 10, Alpha: 0.000003
[1 epoch] of process 11, Alpha: 0.025000
[2 epoch] of process 11, Alpha: 0.001030
[3 epoch] of process 11, Alpha: 0.000003
[4 epoch] of process 11, Alpha: 0.000003
[5 epoch] of process 11, Alpha: 0.000003
[1 epoch] of process 8, Alpha: 0.025000
[2 epoch] of process 8, Alpha: 0.001779
[3 epoch] of process 8, Alpha: 0.000003
[4 epoch] of process 8, Alpha: 0.000003
[5 epoch] of process 8, Alpha: 0.000003
[1 epoch] of process 9, Alpha: 0.025000
[2 epoch] of process 9, Alpha: 0.002528
[3 epoch] of process 9, Alpha: 0.000003
[4 epoch] of process 9, Alpha: 0.000003
[5 epoch] of process 9, Alpha: 0.000003
[1 epoch] of process 7, Alpha: 0.025000
[2 epoch] of process 7, Alpha: 0.001280
[3 epoch] of process 7, Alpha: 0.000003
[4 epoch] of process 7, Alpha: 0.000003
[5 epoch] of process 7, Alpha: 0.000003
[1 epoch] of process 6, Alpha: 0.025000
[2 epoch] of process 6, Alpha: 0.003277
[3 epoch] of process 6, Alpha: 0.000003
[4 epoch] of process 6, Alpha: 0.000003
[5 epoch] of process 6, Alpha: 0.000003
[1 epoch] of process 1, Alpha: 0.025000
[2 epoch] of process 1, Alpha: 0.003527
[3 epoch] of process 1, Alpha: 0.000003
[4 epoch] of process 1, Alpha: 0.000003
[5 epoch] of process 1, Alpha: 0.000003
[1 epoch] of process 4, Alpha: 0.025000
[2 epoch] of process 4, Alpha: 0.000780
[3 epoch] of process 4, Alpha: 0.000003
[4 epoch] of process 4, Alpha: 0.000003
[5 epoch] of process 4, Alpha: 0.000003
data path: /root/nlp/thunlp_guide/data/word2vec/1m_words.txt
save path: /root/nlp/thunlp_guide/model/1m_words.txt_5_skip-gram_neg5.model
